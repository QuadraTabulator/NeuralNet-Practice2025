---
title: "Neural Nets Refresher"
author: "Darryl Fung"
date: "2025-07-17"
output: html_document
---

# Neural nets refresher
Information from https://www.datacamp.com/tutorial/neural-network-models-r

## Packages setup
```{r}
install.packages(c('neuralnet','keras','tensorflow'),dependencies = T)
```

```{r}
library(tidyverse)
library(neuralnet)
```


## Data Analysis

```{r}
iris = iris %>% mutate_if(is.character, as.factor)
```

- Converts columns that are type character into type factor.
- Converts raw character columns into factor columns (so they have categories)

```{r}
summary(iris)
```
Note: all three classes (setosa, versicolor, virginica) have 50 samples. The data is balanced

## Make Train and Test Split
- Usually 70/30 or 80/20 split
- By setting a seed, we can test for reproducibility
```{r}
set.seed(245)
data_rows=floor(0.8*nrow(iris)) #round down

train_indices=sample(
  c(1:nrow(iris)),
  data_rows
)
  # from all the rows of iris...
  # Sample an amount of rows equal to data_rows (aka rounded-down 80% of numrows)
  # NOTE: train_indices is the sampled ROWS

train_data=iris[train_indices,] # new df that's only iris' training rows
test_data=iris[-train_indices,] # new df that EXCLUDES iris' training rows
```
Since we did a 80/20 split, 120 rows are train and 30 rows are test

## Training Neural Net
Of course, we use the training data.
Make a neural net model and plot it
```{r}
model = neuralnet(
    Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,
data=train_data,
hidden=c(4,2),
linear.output = FALSE
)

  # Hidden parameter = vector.
  # In the vector, specify # of neurons/vertices in each layer
```
Reminder: ~ means "in relation to."
Target/dependent variable ~ Predictor/independent variable(s)


```{r}
plot(model,rep = "best") # rep = representation parameter
```
## Model Evaluation
### Confusion Matrix
```{r}
pred <- predict(model, test_data) #make predictions based on our trained model

labels <- c("setosa", "versicolor", "virginica") #label for the three species of flowers

prediction_label <- data.frame(max.col(pred)) %>%     
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()
# remember that max.col.pred is the default name given to the single column in the dataframe that was made from max.col(pred)

# we are finding the maximum value for the row because that's where model's prediction value is highest. (each row = one flower, the model gives a set of probability predictions for each column, i.e. % prob that the value belongs to each flower type.) The highest prediction value is effectively the model's prediction for flower type.


table(test_data$Species, prediction_label)
```
Any off-diagonals that don't indicate match between real and predicted (eg: setosa to setosa)
means the prediction was wrong.
The prediction had 3 predictions where it was actually virginica and it predicted versicolor

```{r}
check = as.numeric(test_data$Species) == max.col(pred)
accuracy = (sum(check)/nrow(test_data))*100
print(accuracy)
```
Model predicts with 90% accuracy


# Convolutional Neural Net (CNN) in R with Keras
- Keras and Tensorflow (Python-based) to build convolutional neural network model for image classification
- cifar10 image dataset

```{r}

library(keras)
library(tensorflow)
install_tensorflow() #creates virtual environment for tensorflow
```


## Training and Test group assignment
```{r}
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_cifar10()
# this assignment operator can't be broken up into newlines for some reason
```
%<-% is made available upon INSTALLING tensorflow, and is unrelated to the assignment operator from zeallot.

```{r}
head(y_train)
```
## Normalization
To normalize data, divide train and test features of x by 255
This changes the values to go on a scale between 0 and 1.
```{r}
x_train <- x_train / 255
x_test <-  x_test / 255
```

```{r}
head(x_train)
```

## Building model (Functional API)
```{r}
# old Sequential style (Datacamp). Doesn't work with newer versions of TensorFlow

# Make an empty keras sequential model.
model <- keras_model_sequential()%>%
  # Start by adding a hidden 2D convolutional layer
  layer_conv_2d(
    filters = 16, kernel_size = c(3,3), padding = "same",
    input_shape = c(32, 32, 3), activation = 'relu'
  ) %>%

  # 2nd hidden layer
  layer_conv_2d(filters = 32, padding="same", kernel_size = c(3,3), activation = 'relu') %>%
 

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%

  # 3rd and 4th hidden 2D convolutional layers
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", activation = 'leaky_relu') %>%

  layer_conv_2d(filter = 64, kernel_size = c(3,3), activation = 'leaky_relu') %>%

  # Use max pooling
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%
 
  # Flatten max filtered output into feature vector
  # and feed into dense layer
  layer_flatten() %>%
  layer_dense(256, activation = 'leaky_relu') %>%
  layer_dropout(0.5) %>%

  # Outputs from dense layer
  layer_dense(10, activation = 'softmax')
```
Tensorflow's Keras as of 2.15 (Nov 2023) uses a different method for adding layers in sequential models. DataCamp's code (Sequential, Feb 2023) is deprecated, so using a new method (Functional API)
https://keras.io/guides/functional_api/
```{r}
# newer versions of TensorFlow render DataCamp's Sequential code unusable.
# Current solutions include $add to directly add layers or use the functional API for %>%-style piping

# Functional API style test:

library(keras)

inputs <- layer_input(shape = c(32, 32, 3))
outputs <- inputs %>%
  layer_conv_2d(filters = 16, kernel_size = c(3,3), padding = "same", activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), padding = "same", activation = "relu")

model <- keras_model(inputs = inputs, outputs = outputs)
summary(model)

```

```{r}
library(keras)

# Input layer
inputs <- layer_input(shape = c(32, 32, 3))

# 1st hidden layer
outputs <- inputs %>%
  layer_conv_2d(
    filters = 16, kernel_size = c(3,3), padding = "same", activation = "relu"
  ) %>%

  # 2nd hidden layer
  layer_conv_2d(
    filters = 32, kernel_size = c(3,3), padding = "same", activation = "relu"
  ) %>%
  
  # Max pooling + dropout
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%

  # 3rd hidden layer
  layer_conv_2d(filters = 32, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu() %>%

  # 4th hidden layer
  layer_conv_2d(filters = 64, kernel_size = c(3,3)) %>%
  layer_activation_leaky_relu() %>%

  # Max pooling + dropout
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.25) %>%

  # Flatten
  layer_flatten() %>%

  # Dense layer
  layer_dense(256) %>%
  layer_activation_leaky_relu() %>%
  layer_dropout(0.5) %>%

  # Output layer
  layer_dense(10, activation = "softmax")

# Build the functional model
model <- keras_model(inputs = inputs, outputs = outputs)

# Check model summary
summary(model)

```

